{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from collections import namedtuple\n",
    "import cv2\n",
    "import os, urllib\n",
    "import numpy as np\n",
    "\n",
    "class ScaleInitializer(mx.init.Initializer):\n",
    "    \"\"\"\n",
    "    Customized initializer for scale layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _init_default(self, name, arr):\n",
    "        if name.endswith(\"scale\"):\n",
    "            self._init_one(name, arr)\n",
    "        else:\n",
    "            raise ValueError('Unknown initialization pattern for %s' % name)\n",
    "            \n",
    "initializer = mx.init.Mixed([\".*scale\", \".*\"], [ScaleInitializer(), mx.init.Xavier(magnitude=1)])\n",
    "\n",
    "def save_net(model, prefix):\n",
    "    model.save_params(prefix+'-0001.params')\n",
    "    model.symbol.save(prefix+'-symbol.json')\n",
    "    \n",
    "def print_net_params(param):\n",
    "    for m in sorted(param):\n",
    "        print m, param[m].shape\n",
    "        \n",
    "def plot_network(symbol, name='network'):\n",
    "    graph = mx.viz.plot_network(symbol=symbol, node_attrs={\"shape\":'rect', \"fixedsize\":'false', 'rankdir': 'TB'})\n",
    "    graph.format = 'png'\n",
    "    graph.render('network.gv', view=True)\n",
    "    \n",
    "def print_inferred_shape(net, name, nch=3, size=300, node_type='rgb'):\n",
    "    if node_type == 'rgb':\n",
    "        ar, ou, au = net.infer_shape(rgb=(1, nch, size, size))\n",
    "    if node_type == 'tir': \n",
    "        ar, ou, au = net.infer_shape(tir=(1, nch, size, size))\n",
    "    if node_type == 'spectral':\n",
    "        ar, ou, au = net.infer_shape(tir=(1, 1, size, size), rgb=(1, nch, size, size))\n",
    "    print ou\n",
    "    \n",
    "def multibox_layer(from_layers, num_classes, sizes=[.2, .95], ratios=[1], normalization=-1, num_channels=[], clip=True, interm_layer=0):\n",
    "    \"\"\"\n",
    "    the basic aggregation module for SSD detection. Takes in multiple layers,\n",
    "    generate multiple object detection targets by customized layers\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    from_layers : list of mx.symbol\n",
    "        generate multibox detection from layers\n",
    "    num_classes : int\n",
    "        number of classes excluding background, will automatically handle\n",
    "        background in this function\n",
    "    sizes : list or list of list\n",
    "        [min_size, max_size] for all layers or [[], [], []...] for specific layers\n",
    "    ratios : list or list of list\n",
    "        [ratio1, ratio2...] for all layers or [[], [], ...] for specific layers\n",
    "    normalizations : int or list of int\n",
    "        use normalizations value for all layers or [...] for specific layers,\n",
    "        -1 indicate no normalizations and scales\n",
    "    num_channels : list of int\n",
    "        number of input layer channels, used when normalization is enabled, the\n",
    "        length of list should equals to number of normalization layers\n",
    "    clip : bool\n",
    "        whether to clip out-of-image boxes\n",
    "    interm_layer : int\n",
    "        if > 0, will add a intermediate Convolution layer\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    list of outputs, as [loc_preds, cls_preds, anchor_boxes]\n",
    "    loc_preds : localization regression prediction\n",
    "    cls_preds : classification prediction\n",
    "    anchor_boxes : generated anchor boxes\n",
    "    \"\"\"\n",
    "    assert len(from_layers) > 0, \"from_layers must not be empty list\"\n",
    "    assert num_classes > 0,      \"num_classes {} must be larger than 0\".format(num_classes)\n",
    "\n",
    "    assert len(ratios) > 0, \"aspect ratios must not be empty list\"\n",
    "    if not isinstance(ratios[0], list):\n",
    "        # provided only one ratio list, broadcast to all from_layers\n",
    "        ratios = [ratios] * len(from_layers)\n",
    "    assert len(ratios) == len(from_layers), \\\n",
    "        \"ratios and from_layers must have same length\"\n",
    "\n",
    "    assert len(sizes) > 0, \"sizes must not be empty list\"\n",
    "    if len(sizes) == 2 and not isinstance(sizes[0], list):\n",
    "        # provided size range, we need to compute the sizes for each layer\n",
    "         assert sizes[0] > 0 and sizes[0] < 1\n",
    "         assert sizes[1] > 0 and sizes[1] < 1 and sizes[1] > sizes[0]\n",
    "         tmp = np.linspace(sizes[0], sizes[1], num=(len(from_layers)-1))\n",
    "         min_sizes = [start_offset] + tmp.tolist()\n",
    "         max_sizes = tmp.tolist() + [tmp[-1]+start_offset]\n",
    "         sizes = zip(min_sizes, max_sizes)\n",
    "    assert len(sizes) == len(from_layers), \\\n",
    "        \"sizes and from_layers must have same length\"\n",
    "\n",
    "    if not isinstance(normalization, list):\n",
    "        normalization = [normalization] * len(from_layers)\n",
    "    assert len(normalization) == len(from_layers)\n",
    "\n",
    "    assert sum(x > 0 for x in normalization) == len(num_channels), \\\n",
    "        \"must provide number of channels for each normalized layer\"\n",
    "\n",
    "    loc_pred_layers = []\n",
    "    cls_pred_layers = []\n",
    "    anchor_layers = []\n",
    "    num_classes += 1 # always use background as label 0\n",
    "\n",
    "    for k, from_layer in enumerate(from_layers):\n",
    "        from_name = from_layer.name\n",
    "        # normalize\n",
    "        if normalization[k] > 0:\n",
    "            from_layer = mx.symbol.L2Normalization(data=from_layer, mode=\"channel\", name=\"{}_norm\".format(from_name))\n",
    "            scale = mx.symbol.Variable(name=\"{}_scale\".format(from_name),shape=(1, num_channels.pop(0), 1, 1))\n",
    "            from_layer = normalization[k] * mx.symbol.broadcast_mul(lhs=scale, rhs=from_layer)\n",
    "        if interm_layer > 0:\n",
    "            from_layer = mx.symbol.Convolution(data=from_layer, kernel=(3,3), stride=(1,1), pad=(1,1), num_filter=interm_layer, name=\"{}_inter_conv\".format(from_name))\n",
    "            from_layer = mx.symbol.Activation(data=from_layer, act_type=\"relu\", name=\"{}_inter_relu\".format(from_name))\n",
    "\n",
    "        # estimate number of anchors per location\n",
    "        # here I follow the original version in caffe\n",
    "        # TODO: better way to shape the anchors??\n",
    "        size = sizes[k]\n",
    "        assert len(size) > 0, \"must provide at least one size\"\n",
    "        size_str = \"(\" + \",\".join([str(x) for x in size]) + \")\"\n",
    "        ratio = ratios[k]\n",
    "        assert len(ratio) > 0, \"must provide at least one ratio\"\n",
    "        ratio_str = \"(\" + \",\".join([str(x) for x in ratio]) + \")\"\n",
    "        num_anchors = len(size) -1 + len(ratio)\n",
    "\n",
    "        # create location prediction layer\n",
    "        num_loc_pred = num_anchors * 4\n",
    "        loc_pred = mx.symbol.Convolution(data=from_layer, kernel=(3,3), stride=(1,1), pad=(1,1), num_filter=num_loc_pred, name=\"{}_loc_pred_conv\".format(from_name))\n",
    "        loc_pred = mx.symbol.transpose(loc_pred, axes=(0,2,3,1))\n",
    "        loc_pred = mx.symbol.Flatten(data=loc_pred)\n",
    "        loc_pred_layers.append(loc_pred)\n",
    "\n",
    "        # create class prediction layer\n",
    "        num_cls_pred = num_anchors * num_classes\n",
    "        cls_pred = mx.symbol.Convolution(data=from_layer, kernel=(3,3), stride=(1,1), pad=(1,1), num_filter=num_cls_pred, name=\"{}_cls_pred_conv\".format(from_name))\n",
    "        cls_pred = mx.symbol.transpose(cls_pred, axes=(0,2,3,1))\n",
    "        cls_pred = mx.symbol.Flatten(data=cls_pred)\n",
    "        cls_pred_layers.append(cls_pred)\n",
    "\n",
    "        # create anchor generation layer\n",
    "        anchors = mx.contrib.symbol.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str, clip=clip, name=\"{}_anchors\".format(from_name))\n",
    "        anchors = mx.symbol.Flatten(data=anchors)\n",
    "        anchor_layers.append(anchors)\n",
    "\n",
    "    loc_preds = mx.symbol.Concat(*loc_pred_layers, num_args=len(loc_pred_layers), dim=1, name=\"multibox_loc_pred\")\n",
    "    cls_preds = mx.symbol.Concat(*cls_pred_layers, num_args=len(cls_pred_layers), dim=1)\n",
    "    cls_preds = mx.symbol.Reshape(data=cls_preds, shape=(0, -1, num_classes))\n",
    "    cls_preds = mx.symbol.transpose(cls_preds, axes=(0, 2, 1), name=\"multibox_cls_pred\")\n",
    "    anchor_boxes = mx.symbol.Concat(*anchor_layers, num_args=len(anchor_layers), dim=1)\n",
    "    anchor_boxes = mx.symbol.Reshape(data=anchor_boxes, shape=(0, -1, 4), name=\"multibox_anchors\")\n",
    "    return [loc_preds, cls_preds, anchor_boxes]\n",
    "\n",
    "    \n",
    "    \n",
    "def bn_act_conv_layer(from_layer, name, num_filter, kernel=(1,1), pad=(0,0), stride=(1,1)):\n",
    "    bn = mx.symbol.BatchNorm(data=from_layer, name=\"bn{}\".format(name))\n",
    "    relu = mx.symbol.Activation(data=bn, act_type='relu')\n",
    "    conv = mx.symbol.Convolution(data=relu, kernel=kernel, pad=pad, stride=stride, num_filter=num_filter, name=\"conv{}\".format(name))\n",
    "    return conv, relu\n",
    "\n",
    "def conv_act_layer(from_layer, name, num_filter, kernel=(1,1), pad=(0,0), stride=(1,1), act_type=\"relu\"):\n",
    "    relu = mx.symbol.Activation(data=from_layer, act_type=act_type, name=\"{}{}\".format(act_type, name))\n",
    "    conv = mx.symbol.Convolution(data=relu, kernel=kernel, pad=pad, stride=stride, num_filter=num_filter, name=\"conv{}\".format(name))\n",
    "    return conv, relu\n",
    "    \n",
    "def residual_unit(data, num_filter, stride, dim_match, name, bn_mom=0.9, workspace=256):\n",
    "    bn1 = mx.sym.BatchNorm(data=data, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn1')\n",
    "    act1 = mx.symbol.Activation(data=bn1, act_type='relu', name=name + '_relu1')\n",
    "    conv1 = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(3, 3), stride=stride, pad=(1, 1),\n",
    "                               no_bias=True, workspace=workspace, name=name + '_conv1')\n",
    "    bn2 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn2')\n",
    "    act2 = mx.symbol.Activation(data=bn2, act_type='relu', name=name + '_relu2')\n",
    "    conv2 = mx.sym.Convolution(data=act2, num_filter=num_filter, kernel=(3, 3), stride=(1, 1), pad=(1, 1),\n",
    "                               no_bias=True, workspace=workspace, name=name + '_conv2')\n",
    "    if dim_match:\n",
    "        shortcut = data\n",
    "    else:\n",
    "        shortcut = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(1, 1), stride=stride, no_bias=True,\n",
    "                                      workspace=workspace, name=name + '_sc')\n",
    "    return conv2 + shortcut\n",
    "\n",
    "\n",
    "# fusion functions\n",
    "def cf_unit(res_unit_rgb, res_unit_tir, num_filters=64, name='fusion1_unit_1', mode='conv'):\n",
    "    if mode == 'conv':\n",
    "        concat = mx.symbol.Concat(*[res_unit_rgb, res_unit_tir], dim=1)\n",
    "        conv = mx.sym.Convolution(concat, num_filter=num_filters, kernel=(3, 3), stride=(1, 1), pad=(1, 1), no_bias=True, workspace=256, name=name + '_conv')\n",
    "        act = mx.symbol.Activation(data=conv, act_type='relu', name=name + '_relu1')\n",
    "\n",
    "    elif mode == 'sum':\n",
    "        conv = mx.symbol.broadcast_add(res_unit_rgb, res_unit_tir, name=name + '_conv')\n",
    "\n",
    "    elif mode == 'max':\n",
    "        conv = mx.symbol.broadcast_maximum(res_unit_rgb, res_unit_tir, name=name + '_conv')\n",
    "    return conv\n",
    "\n",
    "\n",
    "def spectral_net():\n",
    "    filter_list = [32, 32, 64, 128, 256]\n",
    "\n",
    "    tir = mx.sym.Variable(name='tir')\n",
    "\n",
    "    net_tir = mx.sym.Convolution(tir, num_filter=filter_list[0], kernel=(3, 3), stride=(1, 1), pad=(1, 1), name='conv_0')\n",
    "    net_tir = mx.symbol.Activation(net_tir, act_type='relu', name='relu_0')\n",
    "    net_tir = mx.symbol.Pooling(net_tir, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', name='pool_0')\n",
    "\n",
    "    net_tir = mx.sym.Convolution(net_tir, num_filter=filter_list[1], kernel=(3, 3), stride=(1, 1), pad=(1, 1), name='conv_1')\n",
    "    net_tir = mx.symbol.Activation(net_tir, act_type='relu', name='relu_1')\n",
    "    net_tir = mx.symbol.Pooling(net_tir, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', name='pool1')\n",
    "\n",
    "    net_tir = mx.sym.Convolution(net_tir, num_filter=filter_list[2], kernel=(3, 3), stride=(1, 1), pad=(1, 1), name='conv2')\n",
    "    net_tir = mx.symbol.Activation(net_tir, act_type='relu', name='relu2')\n",
    "    net_tir = mx.symbol.Pooling(net_tir, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', name='pool2')\n",
    "\n",
    "    net_tir = mx.sym.Convolution(net_tir, num_filter=filter_list[3], kernel=(3, 3), stride=(1, 1), pad=(1, 1), name='conv3')\n",
    "    net_tir = mx.symbol.Activation(net_tir, act_type='relu', name='relu3')\n",
    "    net_tir = mx.symbol.Pooling(net_tir, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', name='pool3')\n",
    "\n",
    "    net_tir = mx.sym.Convolution(net_tir, num_filter=filter_list[4], kernel=(3, 3), stride=(1, 1), pad=(1, 1), name='conv4')\n",
    "    net_tir = mx.symbol.Activation(net_tir, act_type='relu', name='relu4')\n",
    "    net_tir = mx.symbol.Pooling(net_tir, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', name='pool4')\n",
    "\n",
    "    net_tir, relu9_2t = conv_act_layer(net_tir, \"f4\", 128, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "    net_tir, relu10_2t = conv_act_layer(net_tir, \"f5\", 128, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "\n",
    "    fusion_1 = net_tir.get_internals()[\"pool2_output\"]\n",
    "    fusion_2 = net_tir.get_internals()[\"pool3_output\"]\n",
    "    fusion_3 = net_tir.get_internals()[\"pool4_output\"]\n",
    "    fusion_4 = net_tir.get_internals()[\"convf4_output\"]\n",
    "    fusion_5 = net_tir.get_internals()[\"convf5_output\"]\n",
    "    return [fusion_1, fusion_2, fusion_3, fusion_4, fusion_5]\n",
    "\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    filter_list = [64, 64, 128, 256, 512]\n",
    "\n",
    "    bn_mom = 0.9\n",
    "    workspace = 256\n",
    "\n",
    "    rgb = mx.sym.Variable(name='rgb')\n",
    "\n",
    "    # rgb head\n",
    "    net_rgb = mx.sym.Convolution(rgb, num_filter=filter_list[0], kernel=(7, 7), stride=(2, 2), pad=(3, 3), no_bias=True, name=\"conv0\", workspace=workspace)\n",
    "    net_rgb = mx.symbol.Activation(net_rgb, act_type='relu', name='relu0')\n",
    "    net_rgb = mx.symbol.Pooling(net_rgb, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max')\n",
    "\n",
    "    # stage 1\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[1], (1, 1), False, name='stage1_unit1', workspace=workspace)\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[1], (1, 1), True, name='stage1_unit2', workspace=workspace)\n",
    "\n",
    "    # stage 2\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[2], (2, 2), False, name='stage2_unit1', workspace=workspace)\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[2], (1, 1), True, name='stage2_unit2', workspace=workspace)\n",
    "\n",
    "    # stage 3\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[3], (2, 2), False, name='stage3_unit1', workspace=workspace)\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[3], (1, 1), True, name='stage3_unit2', workspace=workspace)\n",
    "\n",
    "    # stage 4\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[4], (2, 2), False, name='stage4_unit1', workspace=workspace)\n",
    "    net_rgb = residual_unit(net_rgb, filter_list[4], (1, 1), True, name='stage4_unit2', workspace=workspace)\n",
    "\n",
    "    # ssd extra layers\n",
    "    extra_layers_input = net_rgb.get_internals()[\"stage4_unit1_relu1_output\"]  # 19x19\n",
    "    conv8_1, relu8_1 = bn_act_conv_layer(extra_layers_input, \"8_1\", 256, kernel=(1, 1), pad=(0, 0), stride=(1, 1))\n",
    "    conv8_2, relu8_2 = bn_act_conv_layer(conv8_1, \"8_2\", 512, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "    conv9_1, relu9_1 = bn_act_conv_layer(conv8_2, \"9_1\", 128, kernel=(1, 1), pad=(0, 0), stride=(1, 1))\n",
    "    conv9_2, relu9_2 = bn_act_conv_layer(conv9_1, \"9_2\", 256, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "    conv10_1, relu10_1 = bn_act_conv_layer(conv9_2, \"10_1\", 128, kernel=(1, 1), pad=(0, 0), stride=(1, 1))\n",
    "    conv10_2, relu10_2 = bn_act_conv_layer(conv10_1, \"10_2\", 256, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "    pool10 = mx.symbol.Pooling(data=conv10_2, pool_type=\"avg\", global_pool=True, kernel=(1, 1), name='pool10')\n",
    "    net_rgb = pool10\n",
    "\n",
    "    fusion_1 = net_rgb.get_internals()[\"stage3_unit1_relu1_output\"]\n",
    "    fusion_2 = net_rgb.get_internals()[\"stage4_unit1_relu1_output\"]\n",
    "    fusion_3 = conv8_2\n",
    "    fusion_4 = conv9_2\n",
    "    fusion_5 = conv10_2\n",
    "\n",
    "    return [fusion_1, fusion_2, fusion_3, fusion_4, fusion_5, pool10]\n",
    "\n",
    "def fusion_net():\n",
    "    rgb_fusion_layers = resnet()\n",
    "    tir_fusion_layers = spectral_net()\n",
    "    input_1 = cf_unit(rgb_fusion_layers[0], tir_fusion_layers[0], num_filters=128, name='fusion1')\n",
    "    input_2 = cf_unit(rgb_fusion_layers[1], tir_fusion_layers[1], num_filters=256, name='fusion2')\n",
    "    input_3 = cf_unit(rgb_fusion_layers[2], tir_fusion_layers[2], num_filters=512, name='fusion3')\n",
    "    input_4 = cf_unit(rgb_fusion_layers[3], tir_fusion_layers[3], num_filters=256, name='fusion4')\n",
    "    input_5 = cf_unit(rgb_fusion_layers[4], tir_fusion_layers[4], num_filters=256, name='fusion5')\n",
    "    input_6 = rgb_fusion_layers[5]\n",
    "\n",
    "    \n",
    "    from_layers = [input_1, input_2, input_3, input_4, input_5, input_6]\n",
    "    sizes = [[.1], [.2, .276], [.38, .461], [.56, .644], [.74, .825], [.92, 1.01]]\n",
    "    ratios = [[1, 2, .5],\n",
    "              [1, 2, .5, 3, 1. / 3],\n",
    "              [1, 2, .5, 3, 1. / 3],\n",
    "              [1, 2, .5, 3, 1. / 3],\n",
    "              [1, 2, .5, 3, 1. / 3],\n",
    "              [1, 2, .5, 3, 1. / 3]]\n",
    "\n",
    "    normalizations = [20, -1, -1, -1, -1, -1]\n",
    "    num_channels = [128]\n",
    "\n",
    "    loc_preds, cls_preds, anchor_boxes = multibox_layer(from_layers, 1,\n",
    "                                                        sizes=sizes,\n",
    "                                                        ratios=ratios,\n",
    "                                                        normalization=normalizations,\n",
    "                                                        num_channels=num_channels,\n",
    "                                                        clip=True,\n",
    "                                                        interm_layer=0)\n",
    "\n",
    "    \n",
    "    return loc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symb=fusion_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage3_unit1_relu1_output [(1L, 128L, 38L, 38L)]\n",
      "stage4_unit1_relu1_output [(1L, 256L, 19L, 19L)]\n",
      "conv8_2_output [(1L, 512L, 10L, 10L)]\n",
      "conv9_2_output [(1L, 256L, 5L, 5L)]\n",
      "conv10_2_output [(1L, 256L, 3L, 3L)]\n"
     ]
    }
   ],
   "source": [
    "internals = resnet()[-1].get_internals()\n",
    "for output in internals.list_outputs():\n",
    "    if output in [\"stage3_unit1_relu1_output\", \n",
    "                  \"stage4_unit1_relu1_output\", \n",
    "                  'conv8_2_output',\n",
    "                  'conv9_2_output',\n",
    "                  'conv10_2_output']:\n",
    "        print output, internals[output].infer_shape(rgb=(1,1,300,300))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool2_output [(1L, 64L, 38L, 38L)]\n",
      "pool3_output [(1L, 128L, 19L, 19L)]\n",
      "pool4_output [(1L, 256L, 10L, 10L)]\n"
     ]
    }
   ],
   "source": [
    "internals = spectral_net()[-1].get_internals()\n",
    "for output in internals.list_outputs():\n",
    "    if output in ['pool2_output',\n",
    "                  'pool3_output', \n",
    "                  'pool4_output',\n",
    "                  'conv8_2t_output', \n",
    "                  'conv9_2t_output',\n",
    "                  'conv10_2t_output',]:\n",
    "        print output, internals[output].infer_shape(tir=(1,1,300,300))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion1_conv_output [(1L, 128L, 38L, 38L)]\n",
      "fusion2_conv_output [(1L, 256L, 19L, 19L)]\n",
      "fusion3_conv_output [(1L, 512L, 10L, 10L)]\n",
      "fusion4_conv_output [(1L, 256L, 5L, 5L)]\n",
      "fusion5_conv_output [(1L, 256L, 3L, 3L)]\n"
     ]
    }
   ],
   "source": [
    "internals = fusion_net().get_internals()\n",
    "for output in internals.list_outputs():\n",
    "    if output in ['fusion1_conv_output',\n",
    "                  'fusion2_conv_output', \n",
    "                  'fusion3_conv_output', \n",
    "                  'fusion4_conv_output', \n",
    "                  'fusion5_conv_output']:\n",
    "        print output, internals[output].infer_shape(tir=(1,1,300,300), rgb=(1,1,300,300))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read trained ssd-resnet 0.68\n",
    "net_params_ssd = mx.initializer.Load('ssd_300-0300.params')\n",
    "net_params = {}\n",
    "for i in net_params_ssd.param:\n",
    "    if 'loc' in i.split('_'):\n",
    "        continue\n",
    "    if 'pred' in i.split('_'):\n",
    "        continue\n",
    "#     if i.split('_')[0] in ['stage1', \n",
    "#                            'stage2', \n",
    "#                            'stage3',\n",
    "#                            'stage4', 'conv8', 'conv9', 'conv10', 'pool10', 'bn', 'conv0']:\n",
    "    net_params[i] = net_params_ssd.param[i]\n",
    "net_params_ssd.param = net_params\n",
    "# sorted(net_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = mx.mod.Module(symbol=fusion_net(), data_names=['rgb', 'tir'])\n",
    "model.bind(data_shapes=[('rgb', (1, 3, 300, 300)), ('tir', (1, 1, 300, 300))])\n",
    "model.init_params(arg_params=net_params,  allow_missing=True, initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_net(model, 'spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_params_final = mx.initializer.Load('spectral-0001.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bn0_beta': <NDArray 64 @cpu(0)>,\n",
       " 'bn0_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'bn0_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'bn0_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'bn10_1_beta': <NDArray 256 @cpu(0)>,\n",
       " 'bn10_1_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'bn10_1_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'bn10_1_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'bn10_2_beta': <NDArray 128 @cpu(0)>,\n",
       " 'bn10_2_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'bn10_2_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'bn10_2_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'bn8_1_beta': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_1_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_1_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_1_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_2_beta': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_2_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_2_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'bn8_2_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'bn9_1_beta': <NDArray 512 @cpu(0)>,\n",
       " 'bn9_1_gamma': <NDArray 512 @cpu(0)>,\n",
       " 'bn9_1_moving_mean': <NDArray 512 @cpu(0)>,\n",
       " 'bn9_1_moving_var': <NDArray 512 @cpu(0)>,\n",
       " 'bn9_2_beta': <NDArray 128 @cpu(0)>,\n",
       " 'bn9_2_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'bn9_2_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'bn9_2_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'bn_data_beta': <NDArray 3 @cpu(0)>,\n",
       " 'bn_data_gamma': <NDArray 3 @cpu(0)>,\n",
       " 'bn_data_moving_mean': <NDArray 3 @cpu(0)>,\n",
       " 'bn_data_moving_var': <NDArray 3 @cpu(0)>,\n",
       " 'conv0_weight': <NDArray 64x3x7x7 @cpu(0)>,\n",
       " 'conv10_1_bias': <NDArray 128 @cpu(0)>,\n",
       " 'conv10_1_weight': <NDArray 128x256x1x1 @cpu(0)>,\n",
       " 'conv10_2_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv10_2_weight': <NDArray 256x128x3x3 @cpu(0)>,\n",
       " 'conv10_2t_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv10_2t_weight': <NDArray 256x256x3x3 @cpu(0)>,\n",
       " 'conv2_bias': <NDArray 128 @cpu(0)>,\n",
       " 'conv2_weight': <NDArray 128x64x3x3 @cpu(0)>,\n",
       " 'conv3_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv3_weight': <NDArray 256x128x3x3 @cpu(0)>,\n",
       " 'conv4_bias': <NDArray 512 @cpu(0)>,\n",
       " 'conv4_weight': <NDArray 512x256x3x3 @cpu(0)>,\n",
       " 'conv8_1_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv8_1_weight': <NDArray 256x256x1x1 @cpu(0)>,\n",
       " 'conv8_2_bias': <NDArray 512 @cpu(0)>,\n",
       " 'conv8_2_weight': <NDArray 512x256x3x3 @cpu(0)>,\n",
       " 'conv9_1_bias': <NDArray 128 @cpu(0)>,\n",
       " 'conv9_1_weight': <NDArray 128x512x1x1 @cpu(0)>,\n",
       " 'conv9_2_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv9_2_weight': <NDArray 256x128x3x3 @cpu(0)>,\n",
       " 'conv9_2t_bias': <NDArray 256 @cpu(0)>,\n",
       " 'conv9_2t_weight': <NDArray 256x512x3x3 @cpu(0)>,\n",
       " 'conv_0_bias': <NDArray 64 @cpu(0)>,\n",
       " 'conv_0_weight': <NDArray 64x1x3x3 @cpu(0)>,\n",
       " 'conv_1_bias': <NDArray 64 @cpu(0)>,\n",
       " 'conv_1_weight': <NDArray 64x64x3x3 @cpu(0)>,\n",
       " 'fusion1_conv_bias': <NDArray 128 @cpu(0)>,\n",
       " 'fusion1_conv_loc_pred_conv_bias': <NDArray 12 @cpu(0)>,\n",
       " 'fusion1_conv_loc_pred_conv_weight': <NDArray 12x128x3x3 @cpu(0)>,\n",
       " 'fusion1_conv_scale': <NDArray 1x128x1x1 @cpu(0)>,\n",
       " 'fusion1_conv_weight': <NDArray 128x256x3x3 @cpu(0)>,\n",
       " 'fusion2_conv_bias': <NDArray 256 @cpu(0)>,\n",
       " 'fusion2_conv_loc_pred_conv_bias': <NDArray 24 @cpu(0)>,\n",
       " 'fusion2_conv_loc_pred_conv_weight': <NDArray 24x256x3x3 @cpu(0)>,\n",
       " 'fusion2_conv_weight': <NDArray 256x512x3x3 @cpu(0)>,\n",
       " 'fusion3_conv_bias': <NDArray 512 @cpu(0)>,\n",
       " 'fusion3_conv_loc_pred_conv_bias': <NDArray 24 @cpu(0)>,\n",
       " 'fusion3_conv_loc_pred_conv_weight': <NDArray 24x512x3x3 @cpu(0)>,\n",
       " 'fusion3_conv_weight': <NDArray 512x1024x3x3 @cpu(0)>,\n",
       " 'fusion4_conv_bias': <NDArray 256 @cpu(0)>,\n",
       " 'fusion4_conv_loc_pred_conv_bias': <NDArray 24 @cpu(0)>,\n",
       " 'fusion4_conv_loc_pred_conv_weight': <NDArray 24x256x3x3 @cpu(0)>,\n",
       " 'fusion4_conv_weight': <NDArray 256x512x3x3 @cpu(0)>,\n",
       " 'fusion5_conv_bias': <NDArray 256 @cpu(0)>,\n",
       " 'fusion5_conv_loc_pred_conv_bias': <NDArray 24 @cpu(0)>,\n",
       " 'fusion5_conv_loc_pred_conv_weight': <NDArray 24x256x3x3 @cpu(0)>,\n",
       " 'fusion5_conv_weight': <NDArray 256x512x3x3 @cpu(0)>,\n",
       " 'pool10_loc_pred_conv_bias': <NDArray 24 @cpu(0)>,\n",
       " 'pool10_loc_pred_conv_weight': <NDArray 24x256x3x3 @cpu(0)>,\n",
       " 'stage1_unit1_bn1_beta': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn1_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn1_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn1_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn2_beta': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn2_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn2_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_bn2_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit1_conv1_weight': <NDArray 64x64x3x3 @cpu(0)>,\n",
       " 'stage1_unit1_conv2_weight': <NDArray 64x64x3x3 @cpu(0)>,\n",
       " 'stage1_unit1_sc_weight': <NDArray 64x64x1x1 @cpu(0)>,\n",
       " 'stage1_unit2_bn1_beta': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn1_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn1_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn1_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn2_beta': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn2_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn2_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_bn2_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'stage1_unit2_conv1_weight': <NDArray 64x64x3x3 @cpu(0)>,\n",
       " 'stage1_unit2_conv2_weight': <NDArray 64x64x3x3 @cpu(0)>,\n",
       " 'stage2_unit1_bn1_beta': <NDArray 64 @cpu(0)>,\n",
       " 'stage2_unit1_bn1_gamma': <NDArray 64 @cpu(0)>,\n",
       " 'stage2_unit1_bn1_moving_mean': <NDArray 64 @cpu(0)>,\n",
       " 'stage2_unit1_bn1_moving_var': <NDArray 64 @cpu(0)>,\n",
       " 'stage2_unit1_bn2_beta': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit1_bn2_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit1_bn2_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit1_bn2_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit1_conv1_weight': <NDArray 128x64x3x3 @cpu(0)>,\n",
       " 'stage2_unit1_conv2_weight': <NDArray 128x128x3x3 @cpu(0)>,\n",
       " 'stage2_unit1_sc_weight': <NDArray 128x64x1x1 @cpu(0)>,\n",
       " 'stage2_unit2_bn1_beta': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn1_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn1_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn1_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn2_beta': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn2_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn2_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_bn2_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'stage2_unit2_conv1_weight': <NDArray 128x128x3x3 @cpu(0)>,\n",
       " 'stage2_unit2_conv2_weight': <NDArray 128x128x3x3 @cpu(0)>,\n",
       " 'stage3_unit1_bn1_beta': <NDArray 128 @cpu(0)>,\n",
       " 'stage3_unit1_bn1_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'stage3_unit1_bn1_moving_mean': <NDArray 128 @cpu(0)>,\n",
       " 'stage3_unit1_bn1_moving_var': <NDArray 128 @cpu(0)>,\n",
       " 'stage3_unit1_bn2_beta': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit1_bn2_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit1_bn2_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit1_bn2_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit1_conv1_weight': <NDArray 256x128x3x3 @cpu(0)>,\n",
       " 'stage3_unit1_conv2_weight': <NDArray 256x256x3x3 @cpu(0)>,\n",
       " 'stage3_unit1_sc_weight': <NDArray 256x128x1x1 @cpu(0)>,\n",
       " 'stage3_unit2_bn1_beta': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn1_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn1_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn1_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn2_beta': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn2_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn2_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_bn2_moving_var': <NDArray 256 @cpu(0)>,\n",
       " 'stage3_unit2_conv1_weight': <NDArray 256x256x3x3 @cpu(0)>,\n",
       " 'stage3_unit2_conv2_weight': <NDArray 256x256x3x3 @cpu(0)>,\n",
       " 'stage4_unit1_bn1_beta': <NDArray 256 @cpu(0)>,\n",
       " 'stage4_unit1_bn1_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'stage4_unit1_bn1_moving_mean': <NDArray 256 @cpu(0)>,\n",
       " 'stage4_unit1_bn1_moving_var': <NDArray 256 @cpu(0)>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_params_final.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bna(net):\n",
    "    # net = mx.symbol.BatchNorm(net)\n",
    "    net = mx.symbol.LeakyReLU(net, act_type=\"elu\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def conv_bna(net, num_filter, is_pool=False):\n",
    "    if is_pool:\n",
    "        net = mx.symbol.Convolution(net, num_filter=num_filter, kernel=(3, 3), pad=(1, 1), stride=(2, 2))\n",
    "    else:\n",
    "        net = mx.symbol.Convolution(net, num_filter=num_filter, kernel=(3, 3), pad=(1, 1))\n",
    "\n",
    "    net = mx.symbol.BatchNorm(net)\n",
    "    net = mx.symbol.LeakyReLU(net, act_type=\"elu\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def up_bna(net, net_merge, num_filter, num_filter_up, up_type='deconv'):\n",
    "    net = conv_bna(net, num_filter)\n",
    "    net = conv_bna(net, num_filter)\n",
    "\n",
    "    if up_type == 'upsample':\n",
    "        # Nearest Neighbor is best used for categorical data like land-use classification or slope classification.\n",
    "        # The values that go into the grid stay exactly the same, a 2 comes out as a 2 and 99 comes out as 99.\n",
    "        # The value of of the output cell is determined by the nearest cell center on the input grid.\n",
    "        # Nearest Neighbor can be used on continuous data but the results can be blocky.\n",
    "        net = mx.sym.UpSampling(net, scale=2, num_filter=num_filter_up, sample_type='nearest')\n",
    "    elif up_type ==  'deconv':\n",
    "        net = mx.sym.Deconvolution(net, kernel=(2, 2), pad=(0, 0), stride=(2, 2), num_filter=num_filter_up)\n",
    "\n",
    "    net = mx.symbol.Concat(*[net, net_merge], dim=1)\n",
    "    # net = mx.symbol.Concat(net, net_merge, num_args=2, dim=1)\n",
    "    net = bna(net)\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_unet_symbol():\n",
    "    data = mx.sym.Variable('data')\n",
    "    label = mx.sym.Variable('label')\n",
    "\n",
    "    # group 1\n",
    "    net = conv_bna(data, 64)\n",
    "    net_merge_1 = conv_bna(net, 64)\n",
    "    net = conv_bna(net_merge_1, 64, is_pool=True)\n",
    "\n",
    "    # group 2\n",
    "    net = conv_bna(net, 128)\n",
    "    net_merge_2= conv_bna(net, 128)\n",
    "    net = conv_bna(net_merge_2, 128, is_pool=True)\n",
    "\n",
    "    # group 3\n",
    "    net = conv_bna(net, 256)\n",
    "    net_merge_3 = conv_bna(net, 256)\n",
    "    net = conv_bna(net_merge_3, 256, is_pool=True)\n",
    "\n",
    "    # group 4\n",
    "    net = conv_bna(net, 512)\n",
    "    net_merge_4 = conv_bna(net, 512)\n",
    "    net = conv_bna(net_merge_4, 512, is_pool=True)\n",
    "\n",
    "    # up groups\n",
    "    net = up_bna(net, net_merge_4, 1024, 512)\n",
    "    net = up_bna(net, net_merge_3, 512, 256)\n",
    "    net = up_bna(net, net_merge_2, 256, 128)\n",
    "    net = up_bna(net, net_merge_1,   128, 64)\n",
    "\n",
    "    # final group\n",
    "    net = conv_bna(conv_bna(net, 64), 64)\n",
    "    net = mx.symbol.Convolution(net, num_filter=1, kernel=(1, 1))\n",
    "    sigmoid = mx.symbol.Activation(net, act_type='sigmoid', name='sigmoid')\n",
    "\n",
    "    return mx.symbol.LogisticRegressionOutput(sigmoid, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symb=get_unet_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = mx.mod.Module(symbol=get_unet_symbol(), data_names=['data'], label_names=['label'])\n",
    "model.bind(data_shapes=[('data', (1, 3, 400, 400))], label_shapes=[('label', (1,1,400,400))])\n",
    "model.init_params(allow_missing=True, initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
